# -*- coding: utf-8 -*-
"""
Created on Mon Oct  7 18:41:14 2019

@author: obazgir
"""

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from fancyimpute import KNN
from scipy.stats import pearsonr
from sklearn.linear_model import LinearRegression


#%% NRMSE

def NRMSE(Y_Target, Y_Predict):
    """
    Computes the Normalized Root Mean Squared Error (NRMSE) and the R-squared (R²) score.

    The NRMSE is a scale-independent version of the Root Mean Squared Error (RMSE), 
    which allows for comparison across different datasets. The R² score (coefficient of determination) 
    measures how well the predictions explain the variance in the target values.

    Parameters:
    -----------
    Y_Target : array-like
        The actual target values (ground truth).
    Y_Predict : array-like
        The predicted values generated by a model.

    Returns:
    --------
    NRMSE : float
        The Normalized Root Mean Squared Error (NRMSE), which indicates the model's 
        prediction error relative to the variance of the actual values.
        - A lower NRMSE means better model performance.
    R2 : float
        The R-squared (coefficient of determination), which represents how well 
        the model's predictions fit the actual data.
        - `R2 = 1` indicates a perfect fit.
        - `R2 = 0` suggests the model is no better than predicting the mean.
    
    Formula:
    --------
    1. Compute the mean of the true values:
       Ȳ = mean(Y_Target)

    2. Compute the Normalized Root Mean Squared Error (NRMSE):
       NRMSE = sqrt(Σ (Y_Predict - Y_Target)² / Σ (Ȳ - Y_Target)²)

    3. Compute the R² score:
       R² = 1 - (NRMSE)²

    Example:
    --------
    >>> import numpy as np
    >>> Y_Target = [3, 5, 7, 9]
    >>> Y_Predict = [2.8, 5.1, 6.8, 9.2]
    >>> nrmse, r2 = NRMSE(Y_Target, Y_Predict)
    >>> print(f"NRMSE: {nrmse:.4f}, R² Score: {r2:.4f}")
    NRMSE: 0.0806, R² Score: 0.9935

    Notes:
    ------
    - This function assumes that Y_Target and Y_Predict are 1D arrays of equal length.
    - It reshapes the arrays into column vectors to ensure compatibility.
    - If NRMSE is very small, it indicates a strong model fit.
    - If R² is negative, it means the model performs worse than simply predicting the mean.

    """
    # Convert to NumPy arrays
    Y_Target = np.array(Y_Target)
    Y_Predict = np.array(Y_Predict)

    # Reshape to column vectors
    Y_Target = Y_Target.reshape(len(Y_Target), 1)
    Y_Predict = Y_Predict.reshape(len(Y_Predict), 1)

    # Compute mean of true values
    Y_Bar = np.mean(Y_Target)

    # Compute NRMSE
    Nom = np.sum((Y_Predict - Y_Target) ** 2)
    Denom = np.sum((Y_Bar - Y_Target) ** 2)
    MSE = np.mean((Y_Predict - Y_Target)**2)
    NRMSE = np.sqrt(Nom / Denom)

    # Compute R² score
    R2 = 1 - NRMSE ** 2

    return NRMSE, R2



def NMAE(Y_Target, Y_Predict):
    """
    Computes the Normalized Mean Absolute Error (NMAE).

    The NMAE is a scale-independent error metric that normalizes 
    the Mean Absolute Error (MAE) by the variance of the target values. 
    This allows comparison across different datasets.

    Parameters:
    -----------
    Y_Target : array-like
        The actual target values (ground truth).
    Y_Predict : array-like
        The predicted values generated by a model.

    Returns:
    --------
    NormMAE : float
        The Normalized Mean Absolute Error (NMAE), which measures the 
        mean absolute deviation of predictions from actual values, 
        normalized by the deviation from the target mean.
        - A lower NMAE indicates better model performance.

    Formula:
    --------
    1. Compute the mean of the true values:
       Ȳ = mean(Y_Target)

    2. Compute the Normalized Mean Absolute Error (NMAE):
       NMAE = mean(|Y_Predict - Y_Target|) / mean(|Y_Bar - Y_Target|)

    Example:
    --------
    >>> import numpy as np
    >>> Y_Target = [3, 5, 7, 9]
    >>> Y_Predict = [2.8, 5.1, 6.8, 9.2]
    >>> nmae = NMAE(Y_Target, Y_Predict)
    >>> print(f"NMAE: {nmae:.4f}")
    NMAE: 0.0812

    Notes:
    ------
    - This function assumes that Y_Target and Y_Predict are 1D arrays of equal length.
    - It reshapes the arrays into column vectors to ensure compatibility.
    - If NMAE is very small, it indicates a strong model fit.
    - NMAE provides an interpretable error measure that is independent of scale.
    """

    # Convert to NumPy arrays
    Y_Target = np.array(Y_Target)
    Y_Predict = np.array(Y_Predict)

    # Reshape to column vectors
    Y_Target = Y_Target.reshape(len(Y_Target), 1)
    Y_Predict = Y_Predict.reshape(len(Y_Predict), 1)

    # Compute mean of true values
    Y_Bar = np.mean(Y_Target)

    # Compute NMAE
    Nom = np.abs(Y_Predict - Y_Target)
    Denom = np.abs(Y_Bar - Y_Target)
    NormMAE = np.mean(Nom) / np.mean(Denom)

    return NormMAE

    

#%% Random position generation
import math

def Random_position(p):
    """
    Generates a random 2D grid position for each feature.

    This function assigns random (x, y) positions to `p` features 
    in a grid layout of approximately sqrt(p) × sqrt(p). 

    Parameters:
    -----------
    p : int
        The total number of features to be assigned random positions.

    Returns:
    --------
    Pos_mat : list of lists
        A list containing `p` elements, where each element is a list `[x, y]`
        representing the assigned position of a feature in a 2D grid.

    Algorithm:
    ----------
    1. Compute the approximate grid size as `NN = sqrt(p) + 1` to ensure enough space.
    2. Generate a shuffled list of feature indices `[0, 1, ..., p-1]`.
    3. Convert each feature index into 2D grid coordinates:
       - `x = index // NN` (row number)
       - `y = index % NN` (column number)

    Example:
    --------
    >>> Random_position(5)
    [[0, 1], [1, 1], [0, 2], [1, 0], [0, 0]]  # Example random output

    Notes:
    ------
    - This function ensures that feature positions are randomized across the grid.
    - The grid size is slightly larger than sqrt(p) to prevent overflow.
    - The returned positions can be used for visualization or feature mapping.
    """

    # Compute the approximate grid size
    NN = int(math.sqrt(p)) + 1

    # Generate and shuffle feature indices
    Feat_num = np.arange(p)
    np.random.shuffle(Feat_num)

    # Assign random positions in a grid layout
    Pos_mat = []
    for i in range(p):
        Pos_mat.append([int(Feat_num[i] / NN), int(Feat_num[i] % NN)])

    return Pos_mat

    
import numpy as np
import math

def Random_Image_Gen(X, Rand_Pos_mat):
    """
    Generates a random 2D image representation for each sample in `X` using a predefined feature position mapping.

    This function rearranges the feature vectors from `X` into a 2D spatial format based on 
    random feature positions (`Rand_Pos_mat`). It then flattens the generated image into a 
    1D feature vector.

    Parameters:
    -----------
    X : numpy.ndarray
        A 2D NumPy array of shape (N, p), where:
        - `N` is the number of samples (rows).
        - `p` is the number of features (columns).
    Rand_Pos_mat : list of lists
        A list of length `p`, where each element is a list `[x, y]` that specifies the 
        assigned 2D grid position of a feature.

    Returns:
    --------
    X_Gen : numpy.ndarray
        A transformed 2D NumPy array of shape (N, NN²), where:
        - `NN²` is the number of pixels in the generated square image (NN × NN).
        - Each row represents a flattened image corresponding to a sample in `X`.

    Algorithm:
    ----------
    1. Compute `NN`, the approximate square grid size needed for `p` features.
    2. Create an empty image matrix `Im` of shape (NN, NN).
    3. For each sample in `X`:
       - Assign feature values to the corresponding (x, y) positions in `Im`.
       - Flatten the 2D image into a 1D vector and store it in `X_Gen`.
    4. Return `X_Gen`, which contains the transformed image-based feature representations.

    Example:
    --------
    >>> X = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])  # 2 samples, 4 features
    >>> Rand_Pos_mat = [[0, 0], [0, 1], [1, 0], [1, 1]]  # 2x2 grid
    >>> X_Gen = Random_Image_Gen(X, Rand_Pos_mat)
    >>> print(X_Gen)
    [[1. 2. 3. 4.]
     [5. 6. 7. 8.]]

    Notes:
    ------
    - This function assumes `Rand_Pos_mat` provides unique positions for each feature.
    - The generated "image" is stored as a **flattened 1D vector**, but it can be reshaped back to NN × NN.
    - The function is useful for transforming tabular data into an image-like structure for CNN-based models.
    """

    # Get dimensions of input data
    sz = X.shape
    p = sz[1]  # Number of features
    N = sz[0]  # Number of samples

    # Compute the approximate grid size for arranging features
    NN = int(math.sqrt(p)) + 1  

    # Initialize the output data structure
    X_Gen = np.zeros((N, NN**2))  

    # Loop through each sample in X
    for j in range(N):
        Im = np.zeros((NN, NN))  # Initialize an empty image matrix for each sample

        for i in range(p):
            P = Rand_Pos_mat[i]  # Get assigned position of feature i
            Im[P[0], P[1]] = X[j, i]  # Assign feature value to corresponding pixel

        # Flatten the image into a 1D vector and store it
        Image_Store = Im.reshape((NN**2, 1)).T  
        X_Gen[j, :] = Image_Store  

    return X_Gen


#%% MDS by Ruibo
from sklearn.manifold import MDS

def two_d_norm(xy):
    """
    Normalize 2D coordinates to the range [0, 1].

    Parameters:
    ----------
    xy : numpy.ndarray
        A 2D NumPy array of shape (N, 2), where N is the number of data points.
        Each row represents a point with x- and y-coordinates.

    Returns:
    -------
    numpy.ndarray
        A 2D NumPy array of the same shape as `xy`, where each coordinate has been
        normalized independently to the range [0, 1].

    Formula:
    --------
    Let `xy[:, 0]` and `xy[:, 1]` represent the x- and y-coordinates, respectively.
    The normalization formula is applied independently to each coordinate:
    
        normalized_x = (x - x_min) / (x_max - x_min)
        normalized_y = (y - y_min) / (y_max - y_min)

    where:
        - x_min, y_min: Minimum values of x and y coordinates, respectively.
        - x_max, y_max: Maximum values of x and y coordinates, respectively.

    Example:
    --------
    >>> import numpy as np
    >>> xy = np.array([[2, 8], [4, 6], [6, 4], [8, 2]])
    >>> two_d_norm(xy)
    array([[0.        , 1.        ],
           [0.33333333, 0.66666667],
           [0.66666667, 0.33333333],
           [1.        , 0.        ]])

    Notes:
    ------
    - The normalization is performed independently for each axis (x and y).
    - Input data must be numeric; non-numeric values may raise an error.

    """
    norm_xy = (xy - xy.min(axis=0)) / (xy - xy.min(axis=0)).max(axis=0)
    return norm_xy


def two_d_eq(xy, Nn):
    """
    Assigns evenly spaced normalized ranks to 2D coordinates based on their order.

    This function takes a 2D array of coordinates and assigns normalized ranks
    to the x- and y-coordinates independently. The ranks are evenly spaced
    in the range [0, 1] based on the sorted order of the x- and y-coordinates.

    Parameters:
    ----------
    xy : numpy.ndarray
        A 2D NumPy array of shape (N, 2), where N is the number of points.
        Each row represents a point with x- and y-coordinates.
    Nn : int
        The total number of points (usually `xy.shape[0]`).

    Returns:
    -------
    numpy.ndarray
        A 2D NumPy array of the same shape as `xy`. Each coordinate is replaced
        by its normalized rank, where the ranks are evenly spaced between 0 and 1.

    Formula:
    --------
    For each coordinate in `xy`:
        - Rank the x-coordinates (ascending order) and assign:
          eq_xy[xx_rank[i], 0] = i / Nn
        - Rank the y-coordinates (ascending order) and assign:
          eq_xy[yy_rank[i], 1] = i / Nn
    where `xx_rank` and `yy_rank` are the indices of the sorted x- and y-coordinates.

    Example:
    --------
    >>> import numpy as np
    >>> xy = np.array([[8, 2], [4, 6], [6, 4], [2, 8]])
    >>> Nn = xy.shape[0]
    >>> two_d_eq(xy, Nn)
    array([[0.75, 0.  ],
           [0.25, 0.5 ],
           [0.5 , 0.25],
           [0.  , 0.75]])

    Notes:
    ------
    - The function sorts the x- and y-coordinates independently.
    - Each coordinate is assigned a rank normalized to the range [0, 1].
    - Input data must be numeric; non-numeric values will raise an error.
    """
    xx_rank = np.argsort(xy[:, 0])
    yy_rank = np.argsort(xy[:, 1])
    eq_xy = np.full(xy.shape, np.nan)
    for ii in range(xy.shape[0]):
        xx_idx = xx_rank[ii]
        yy_idx = yy_rank[ii]
        eq_xy[xx_idx, 0] = ii * 1 / Nn
        eq_xy[yy_idx, 1] = ii * 1 / Nn
    return eq_xy


#embedding = MDS(n_components=2)
#mds_xy = embedding.fit_transform(transposed_input)
# to pixels

from itertools import product
from sklearn.metrics import pairwise_distances


def Assign_features_to_pixels(xy,nn,verbose = False):
    """
    Assigns 2D features to pixels on a grid based on proximity.

    This function takes a set of 2D feature coordinates and a grid size, 
    and assigns each feature to the nearest available pixel on the grid. 
    If multiple features compete for the same pixel, the function resolves 
    conflicts by prioritizing the feature that is closest to the pixel.

    Parameters:
    ----------
    xy : numpy.ndarray
        A 2D NumPy array of shape (N, 2), where N is the number of features.
        Each row represents a feature with x- and y-coordinates normalized 
        to the range [0, 1].
    nn : int
        The width and height of the grid (e.g., for a 3x3 grid, nn=3). 
        The grid has nn * nn pixels.
    verbose : bool, optional
        If True, prints progress during feature assignment.

    Returns:
    -------
    numpy.ndarray
        A 2D NumPy array of shape (nn, nn), where each pixel contains the label 
        of the assigned feature (e.g., 'F0', 'F1', etc.). Pixels without an 
        assigned feature are labeled as 'NaN'.

    Algorithm Steps:
    ----------------
    1. Initialize a table (`result_table`) to store pixel coordinates and their assignments.
    2. Compute the centroids of all pixels in normalized space.
    3. Calculate the Euclidean distance between each feature and each pixel centroid.
    4. Assign each feature to the nearest available pixel:
        - If a feature's nearest pixel is unoccupied, assign the feature to that pixel.
        - If a pixel is contested by multiple features, assign the nearest feature.
    5. Update the grid with feature labels (e.g., 'F0', 'F1').

    Example:
    --------
    >>> import numpy as np
    >>> xy = np.array([[0.1, 0.8], [0.3, 0.3], [0.9, 0.9], [0.6, 0.2]])
    >>> nn = 3
    >>> img = Assign_features_to_pixels(xy, nn, verbose=True)
    >>> print(img)
    [['F1', 'NaN', 'F0'],
     ['F3', 'NaN', 'NaN'],
     ['NaN', 'NaN', 'F2']]

    Notes:
    ------
    - The input `xy` must contain normalized coordinates in the range [0, 1].
    - If verbose mode is enabled, the function prints the number of features 
      assigned at each iteration.

    """
    # For each unassigned feature, find its nearest pixel, repeat until every ft is assigned
    # xy is the 2-d coordinates (normalized to [0,1]); nn is the image width. Img size = n x n
    # generate the result summary table, xy pixels; 3rd is nan for filling the idx


    Nn = xy.shape[0]
    
    
    pixel_iter = product([x for x in range(nn)],repeat = 2)
    result_table = np.full((nn*nn,3),np.nan)
    ii = 0
    for each_pixel in pixel_iter:
        result_table[ii,:2] = np.array(each_pixel)
        ii+=1
    # Use numpy array for speed 
        
    
#    xy = eq_xy
    centroids = result_table[:,:2] / nn + 0.5/nn
    pixel_avail = np.ones(nn*nn).astype(bool)
    feature_assigned = np.zeros(Nn).astype(bool)
    
    dist_xy_centroids = pairwise_distances(centroids,xy,metric='euclidean')
    
    while feature_assigned.sum()<Nn:
        # Init the pick-relationship table
        pick_xy_centroids = np.zeros(dist_xy_centroids.shape).astype(bool)
        
        for each_feature in range(Nn):
            # for each feature, find the nearest available pixel
            if feature_assigned[each_feature] == True:
                # if this feature is already assigned, skip to the next ft
                continue
            else:
                # if not assigned:
                for ii in range(nn*nn):
                    # find the nearest avail pixel
                    nearest_pixel_idx = np.argsort(dist_xy_centroids[:,each_feature])[ii]
                    if pixel_avail[nearest_pixel_idx] == True:
                        break
                    else:
                        continue
                pick_xy_centroids[nearest_pixel_idx,each_feature] = True
            
        for each_pixel in range(nn*nn):
            # Assign the feature No to pixels
            if pixel_avail[each_pixel] == False:
                continue
            else:
                # find all the "True" features. np.where returns a tuple size 1
                related_features = np.where(pick_xy_centroids[each_pixel,:] == 1)[0]
                if len(related_features) == 1:
                    # Assign it
                    result_table[each_pixel,2] = related_features[0]
                    pixel_avail[each_pixel] = False
                    feature_assigned[related_features[0]] = True
                elif len(related_features) > 1:
                    related_dists = dist_xy_centroids[each_pixel,related_features]
                    best_feature = related_features[np.argsort(related_dists)[0]] # Sort, and pick the nearest one among them
                    result_table[each_pixel,2] = best_feature
                    pixel_avail[each_pixel] = False
                    feature_assigned[best_feature] = True
        if verbose:
            print(">> Assign features to pixels:", feature_assigned.sum(),"/",Nn)
    result_table = result_table.astype(int)
    
    img = np.full((nn,nn),'NaN').astype(object)
    for each_pixel in range(nn*nn):
        xx = result_table[each_pixel,0]
        yy = result_table[each_pixel,1]
        ft = 'F' + str(result_table[each_pixel,2])
        img[xx,yy] = ft
    return img.astype(object)
print(">>>> MDS")
#eq_xy = two_d_eq(mds_xy)
#Img = Assign_features_to_pixels(eq_xy,nn,verbose=1)
#Init_Corr_MDS = InitCorr(dist_mat,Img,nn)

import numpy as np

def MDS_Im_Gen(X, nn, Img):
    """
    Generates a structured image representation of input data based on Multi-Dimensional Scaling (MDS).

    This function takes a dataset `X` and rearranges its features into a 2D structured format based 
    on an image representation `Img`. The generated image structure is then flattened into a 1D vector.

    Parameters:
    -----------
    X : numpy.ndarray
        A 2D NumPy array of shape (N_sam, P_Feat), where:
        - `N_sam` is the number of samples (rows).
        - `P_Feat` is the number of features (columns).
    nn : int
        The width/height of the square grid (image size is `nn x nn`).
    Img : numpy.ndarray
        A 2D NumPy array of shape (nn, nn), where each entry contains a feature label (e.g., "F3").

    Returns:
    --------
    X_Gen : numpy.ndarray
        A transformed 2D NumPy array of shape (N_sam, nn²), where:
        - Each row represents a flattened image corresponding to a sample in `X`.
        - Missing or out-of-bound features are assigned a value of 0.

    Algorithm:
    ----------
    1. Compute the number of samples (`N_sam`) and features (`P_Feat`) in `X`.
    2. Initialize an output matrix `X_Gen` of shape (N_sam, nn²) with zeros.
    3. Flatten the input `Img` into a 1D vector (`conv_Img`).
    4. For each pixel in the image:
       - Extract the feature number from `Img` (e.g., "F3" → 3).
       - If the feature index is valid, copy the corresponding feature values from `X`.
       - Otherwise, assign a value of 0 to handle missing or out-of-bounds features.
    5. Return `X_Gen`, which contains the transformed image-based feature representations.

    Example:
    --------
    >>> X = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])  # 2 samples, 4 features
    >>> Img = np.array([["F0", "F1"], ["F2", "F3"]])  # 2x2 grid mapping
    >>> X_Gen = MDS_Im_Gen(X, 2, Img)
    >>> print(X_Gen)
    [[1. 2. 3. 4.]
     [5. 6. 7. 8.]]

    Notes:
    ------
    - `Img` is expected to contain feature labels in the format "F<number>".
    - The function assumes `X` has at least as many features as referenced in `Img`.
    - If an invalid feature index is encountered, the function assigns a value of 0.
    """

    # Get the number of samples and features in X
    N_sam, P_Feat = X.shape  

    # Initialize the output transformed matrix
    X_Gen = np.zeros((N_sam, nn**2))  

    # Flatten the image representation into a 1D array
    conv_Img = Img.reshape(Img.size, 1)  

    # Iterate through each pixel position
    for i in range(nn**2):
        Feature = np.array(conv_Img[i])  
        Feature = Feature[0]  # Extract feature string (e.g., "F3")
        F_Num = int(Feature[1:])  # Extract feature index (e.g., 3 from "F3")

        if abs(F_Num) < nn**2:
            X_Gen[:, i] = X[:, F_Num]  # Assign feature values
        else:
            X_Gen[:, i] = 0  # Assign zero for missing features

    return X_Gen


#%% CCLE functions

def dataframer(Main, Set_in, name_in, name_out):
    """
    Filters a DataFrame (`Main`) based on matching values from another DataFrame (`Set_in`).

    This function extracts rows from `Main` where the values in the `name_out` column match
    any value from the `name_in` column of `Set_in`.

    Parameters:
    -----------
    Main : pandas.DataFrame
        The primary DataFrame containing all records.
    Set_in : pandas.DataFrame
        A DataFrame containing values to filter `Main`.
    name_in : str
        The column name in `Set_in` whose values will be used for filtering.
    name_out : str
        The column name in `Main` that will be compared against `Set_in[name_in]` values.

    Returns:
    --------
    Set_out : pandas.DataFrame
        A new DataFrame containing only rows from `Main` where `name_out` matches any value in `Set_in[name_in]`.

    Algorithm:
    ----------
    1. Extract values from `Set_in[name_in]` as a list (`A`).
    2. Find rows in `Main` where `name_out` matches the first value in `A`.
    3. Iterate through the rest of `A`, concatenating matching rows to `Set_out`.
    4. Return the filtered DataFrame.

    Example:
    --------
    >>> data = {'ID': [1, 2, 3, 4, 5], 'Category': ['A', 'B', 'A', 'C', 'B']}
    >>> df_main = pd.DataFrame(data)

    >>> filter_values = {'Filter_Column': ['A', 'C']}
    >>> df_filter = pd.DataFrame(filter_values)

    >>> result = dataframer(df_main, df_filter, 'Filter_Column', 'Category')
    >>> print(result)

       ID Category
    0   1       A
    2   3       A
    3   4       C

    Notes:
    ------
    - This function performs a **manual iterative filtering**, which can be optimized.
    - If `Set_in[name_in]` contains unique values, `pd.merge()` or `pd.isin()` may be more efficient.
    """

    # Extract values from Set_in[name_in] into a list
    A = Set_in[name_in].tolist()

    # Initialize the output DataFrame with the first match
    Set_out = Main[Main[name_out] == A[0]]

    # Iterate through remaining values and concatenate matching rows
    for cell in range(len(A) - 1):
        df = Main[Main[name_out] == A[cell + 1]]
        Set_out = pd.concat([Set_out, df])

    return Set_out


import numpy as np

def Reg_to_Class(Y, Threshold):
    """
    Converts a continuous regression output into a binary classification.

    This function applies a threshold to a continuous numerical array (`Y`), 
    converting values greater than the threshold to `1` and others to `0`.

    Parameters:
    -----------
    Y : array-like
        A 1D NumPy array or list of continuous numerical values (regression outputs).
    Threshold : float
        A threshold value used to classify the input values.

    Returns:
    --------
    Y_Class : numpy.ndarray
        A 1D binary NumPy array where:
        - `1` indicates values greater than `Threshold`.
        - `0` indicates values less than or equal to `Threshold`.

    Algorithm:
    ----------
    1. Initialize `Y_Class` as an array of zeros (same length as `Y`).
    2. Find indices where `Y > Threshold` and set them to `1`.
    3. Convert `Y_Class` to an integer array.
    4. Convert it to a NumPy array before returning.

    Example:
    --------
    >>> Y = np.array([0.1, 0.5, 1.2, 3.0, 0.9])
    >>> Threshold = 1.0
    >>> Y_Class = Reg_to_Class(Y, Threshold)
    >>> print(Y_Class)
    [0 0 1 1 0]

    Notes:
    ------
    - This function is useful for **converting regression outputs into classification labels**.
    - If `Y` contains missing values (`NaN`), consider preprocessing before applying this function.
    - The function assumes `Y` is a 1D array; if `Y` is multi-dimensional, flatten it first.
    """

    # Initialize an array of zeros (default classification)
    Y_Class = np.zeros(len(Y))

    # Find indices where Y > Threshold and set them to 1
    Y_Sens = np.where(Y > Threshold)
    Y_Class[Y_Sens] = 1

    # Convert to integer and return as a NumPy array
    return Y_Class.astype(int)

	
def floattoint(Y_Test_Encoded):
    """
    Converts a floating-point probability array into binary classification (0 or 1).

    This function takes an array of continuous values (e.g., probabilities from a model)
    and applies a **0.5 threshold**, converting values greater than `0.5` to `1` and 
    values less than or equal to `0.5` to `0`.

    Parameters:
    -----------
    Y_Test_Encoded : numpy.ndarray
        A NumPy array containing floating-point values (e.g., probabilities).

    Returns:
    --------
    Y_Class : numpy.ndarray
        A binary NumPy array where:
        - Values > `0.5` → `1`
        - Values ≤ `0.5` → `0`

    Algorithm:
    ----------
    1. Initialize `Y_Class` as an array of zeros with the same shape as `Y_Test_Encoded`.
    2. Identify indices where `Y_Test_Encoded > 0.5` and assign `1` at those positions.
    3. Convert the array to integer type (`int`).
    4. Convert it to a NumPy array before returning.

    Example:
    --------
    >>> Y_Test_Encoded = np.array([0.2, 0.6, 0.9, 0.4, 0.7])
    >>> Y_Class = floattoint(Y_Test_Encoded)
    >>> print(Y_Class)
    [0 1 1 0 1]

    Notes:
    ------
    - This function is typically used after applying **sigmoid activation** in binary classification models.
    - If your model outputs soft probabilities, this function **binarizes the predictions**.
    - The function assumes `Y_Test_Encoded` is a NumPy array; if it's a list, convert it first.
    """

    # Initialize an array of zeros (default classification)
    Y_Class = np.zeros(Y_Test_Encoded.shape)

    # Find indices where values are greater than 0.5 and set them to 1
    Y_Sens = np.where(Y_Test_Encoded > 0.5)
    Y_Class[Y_Sens] = 1

    # Convert to integer type and return as a NumPy array
    return Y_Class.astype(int)


def REFINED_Im_Gen(X, nn, map_in_int, gene_names, coords):
    """
    Generates an image representation of input data using the REFINED method.

    This function transforms tabular data (`X`) into a structured 2D image representation
    based on a predefined mapping (`coords`). Each feature in `X` is assigned a spatial 
    position in the image, creating a grid-like structure that can be used for 
    convolutional neural networks (CNNs).

    Parameters:
    -----------
    X : numpy.ndarray
        A 2D NumPy array of shape (N_sam, P_Feat), where:
        - `N_sam` is the number of samples (rows).
        - `P_Feat` is the number of features (columns).
    nn : int
        The width/height of the square grid (image size is `nn x nn`).
    map_in_int : numpy.ndarray
        A 2D NumPy array defining the spatial structure of the grid.
    gene_names : list
        A list of `P_Feat` feature names corresponding to the columns of `X`.
    coords : numpy.ndarray
        A 2D NumPy array of shape (P_Feat, 2), where each row `[x, y]` specifies
        the spatial coordinates of the corresponding feature.

    Returns:
    --------
    X_Gen : numpy.ndarray
        A transformed 2D NumPy array of shape (N_sam, nn²), where:
        - Each row represents a flattened image corresponding to a sample in `X`.
        - Feature values are placed according to the spatial mapping in `coords`.

    Algorithm:
    ----------
    1. Get the number of samples (`N_sam`) and features (`P_Feat`) from `X`.
    2. Initialize an output array `X_Gen` of shape (N_sam, nn²) filled with zeros.
    3. Iterate through each sample in `X`:
       - Convert feature values into a DataFrame (`X_REFINED`) using `gene_names` as column names.
       - Initialize an empty `Image` of shape `(nn, nn)`.
       - Assign feature values to their respective coordinates (`coords[j,0], coords[j,1]`).
       - Flatten the image and store it in `X_Gen`.
    4. Return `X_Gen`, containing the structured feature representation.

    Example:
    --------
    >>> X = np.array([[0.5, 1.2, 0.8], [0.3, 1.0, 0.6]])  # 2 samples, 3 features
    >>> map_in_int = np.zeros((2, 2))  # 2x2 image grid
    >>> gene_names = ['Gene1', 'Gene2', 'Gene3']
    >>> coords = np.array([[0, 0], [0, 1], [1, 0]])  # Assign features to (x, y) positions
    >>> X_Gen = REFINED_Im_Gen(X, 2, map_in_int, gene_names, coords)
    >>> print(X_Gen)
    [[0.5 1.2 0.8 0.0]
     [0.3 1.0 0.6 0.0]]

    Notes:
    ------
    - `map_in_int` is not directly used but can define the spatial layout of the features.
    - `gene_names` should match the feature names in `X` for correct mapping.
    - `coords` must have the same number of rows as features in `X` (i.e., `P_Feat` rows).
    - The function ensures that feature values are positioned correctly in the output image.
    """

    # Get the number of samples and features
    N_sam, P_Feat = X.shape  

    # Initialize the output transformed matrix
    X_Gen = np.zeros((N_sam, nn**2))  

    # Loop through each sample in X
    for i in range(N_sam):
        # Convert the feature vector into a DataFrame for easy lookup
        data = X[i, :]  
        X_REFINED = pd.DataFrame(data=data.reshape(1, len(data)), columns=gene_names)

        # Initialize an empty image
        Image = np.zeros(map_in_int.shape)  

        # Assign feature values to their respective coordinates in the image
        for j in range(len(coords)):
            val = np.array(X_REFINED[gene_names[j]])  # Extract feature value
            Image[coords[j, 0], coords[j, 1]] = val  

        # Flatten the image and store it in the output matrix
        Image = Image.reshape(nn**2)  
        X_Gen[i, :] = Image  

    return X_Gen

#%% GDSC

def GDSC_dataframer(PD_Set, Set_Name, PD_Attribute, Attribute_Name):
    """
    Extracts and aligns data from a primary dataset (`PD_Attribute`) based on matching values 
    from a filtering dataset (`PD_Set`), creating a structured numerical array and a DataFrame.

    This function takes a set of identifiers from `PD_Set[Set_Name]` and extracts corresponding 
    rows from `PD_Attribute` where `Attribute_Name` matches these identifiers. The selected 
    numerical data is stored as both a NumPy array and a Pandas DataFrame.

    Parameters:
    -----------
    PD_Set : pandas.DataFrame
        A DataFrame containing a column (`Set_Name`) with identifiers used for filtering.
    Set_Name : str
        The column name in `PD_Set` that contains the identifiers for filtering.
    PD_Attribute : pandas.DataFrame
        The primary DataFrame containing numerical attributes for each identifier.
    Attribute_Name : str
        The column name in `PD_Attribute` used for matching identifiers.

    Returns:
    --------
    Data_arry : numpy.ndarray
        A NumPy array containing the extracted numerical data, with shape (num_identifiers, num_features).
    PD_Data_arry : pandas.DataFrame
        A DataFrame version of `Data_arry`, preserving the column names and indexed by the identifiers.

    Algorithm:
    ----------
    1. Extract the list of identifiers (`A`) from `PD_Set[Set_Name]`.
    2. Find the first matching row in `PD_Attribute` and store it as a NumPy array.
    3. Iterate through the remaining identifiers, extracting and appending matching rows.
    4. Convert the final NumPy array into a Pandas DataFrame with appropriate column names and index.
    5. Return both the NumPy array (`Data_arry`) and its DataFrame representation (`PD_Data_arry`).

    Example:
    --------
    >>> pd_set = pd.DataFrame({'Cell_Lines': ['A549', 'MCF7', 'HCT116']})
    >>> pd_attribute = pd.DataFrame({
    ...     'Cell_Lines': ['A549', 'MCF7', 'HCT116'],
    ...     'Feature1': [0.5, 1.2, 0.7],
    ...     'Feature2': [0.8, 0.6, 1.1]
    ... })
    >>> data_array, df_output = GDSC_dataframer(pd_set, 'Cell_Lines', pd_attribute, 'Cell_Lines')
    >>> print(data_array)
    [[0.5 0.8]
     [1.2 0.6]
     [0.7 1.1]]

    >>> print(df_output)
            Feature1  Feature2
    A549       0.5      0.8
    MCF7       1.2      0.6
    HCT116     0.7      1.1

    Notes:
    ------
    - This function assumes that all identifiers in `PD_Set[Set_Name]` exist in `PD_Attribute[Attribute_Name]`.
    - The function extracts numerical values only (skipping the identifier column).
    - The DataFrame representation (`PD_Data_arry`) is indexed by the identifiers.
    """

    # Extract the list of identifiers from PD_Set
    A = PD_Set[Set_Name].tolist()

    # Get the first matching row and convert it to a NumPy array
    b = PD_Attribute[PD_Attribute[Attribute_Name] == A[0]].reset_index().drop(columns=['index'])
    Data_arry = np.array(b.values[0, 1:], dtype=float).reshape(1, -1)

    # Iterate through the remaining identifiers
    for i in range(len(A) - 1):
        b = PD_Attribute[PD_Attribute[Attribute_Name] == A[i + 1]].reset_index().drop(columns=['index'])
        Arr = np.array(b.values[0, 1:], dtype=float).reshape(1, -1)
        Data_arry = np.append(Data_arry, Arr, axis=0)

    # Convert the NumPy array to a Pandas DataFrame
    PD_Data_arry = pd.DataFrame(data=Data_arry, columns=PD_Attribute.columns.tolist()[1:], index=A)

    return Data_arry, PD_Data_arry

	

def GDSC_NPier(PD_Set, Set_Name, PD_Attribute, Attribute_Name):
    """
    Maps attribute values from `PD_Attribute` to corresponding identifiers in `PD_Set` 
    and returns a NumPy array representation.

    This function matches unique identifiers from `PD_Set[Set_Name]` with values from
    `PD_Attribute[Attribute_Name]` and assigns the corresponding feature values to a 
    structured NumPy array.

    Parameters:
    -----------
    PD_Set : pandas.DataFrame
        A DataFrame containing a column (`Set_Name`) with unique identifiers.
    Set_Name : str
        The column name in `PD_Set` that contains identifiers used for mapping.
    PD_Attribute : pandas.DataFrame
        A DataFrame containing numerical attribute values corresponding to the identifiers.
    Attribute_Name : str
        The column name in `PD_Attribute` used for matching identifiers.

    Returns:
    --------
    X_NP : numpy.ndarray
        A 2D NumPy array where:
        - Rows correspond to entries in `PD_Set`.
        - Columns correspond to the feature values from `PD_Attribute`.

    Algorithm:
    ----------
    1. Reset the index of `PD_Set` to ensure proper indexing.
    2. Determine the number of rows in `PD_Set` and the number of features in `PD_Attribute`.
    3. Initialize a zero-filled NumPy array `X_NP` of appropriate shape.
    4. Extract unique identifiers from `PD_Set[Set_Name]`.
    5. Iterate through each unique identifier:
       - Find the matching row index in `PD_Set`.
       - Retrieve the corresponding feature values from `PD_Attribute`.
       - Assign them to the correct row in `X_NP`.
    6. Return `X_NP`.

    Example:
    --------
    >>> pd_set = pd.DataFrame({'Cell_Lines': ['A549', 'MCF7', 'HCT116']})
    >>> pd_attribute = pd.DataFrame({
    ...     'Cell_Lines': ['A549', 'MCF7', 'HCT116'],
    ...     'Feature1': [0.5, 1.2, 0.7],
    ...     'Feature2': [0.8, 0.6, 1.1]
    ... })
    >>> X_NP = GDSC_NPier(pd_set, 'Cell_Lines', pd_attribute, 'Cell_Lines')
    >>> print(X_NP)
    [[0.5 0.8]
     [1.2 0.6]
     [0.7 1.1]]

    Notes:
    ------
    - `PD_Attribute` must contain all unique identifiers present in `PD_Set`.
    - The function assumes that each identifier appears only once in `PD_Attribute`.
    - The resulting NumPy array maintains the row order of `PD_Set`.
    """

    # Reset index to avoid indexing issues
    PD_Set = PD_Set.reset_index()

    # Get the number of samples (rows) and features (excluding the identifier column)
    num_samples = PD_Set.shape[0]
    num_features = PD_Attribute.shape[1] - 1

    # Initialize a NumPy array to store extracted feature values
    X_NP = np.zeros((num_samples, num_features))

    # Extract unique identifiers from PD_Set
    Source = list(set(PD_Set[Set_Name].tolist()))

    # Iterate through each unique identifier and map feature values
    for name in Source:
        # Find indices of occurrences of `name` in `PD_Set`
        idx = PD_Set.index[PD_Set[Set_Name] == name].tolist()

        # Retrieve feature values from `PD_Attribute` and convert them to float
        XX = np.array(PD_Attribute[PD_Attribute[Attribute_Name] == name].values[0, 1:], dtype=float)

        # Assign extracted values to the corresponding row(s) in X_NP
        X_NP[idx, :] = XX

    return X_NP

def optimized_GDSC_NPier(PD_Set, Set_Name, PD_Attribute, Attribute_Name):
    """
    Optimized version using pandas' merge function.
    """
    merged_df = PD_Attribute.merge(PD_Set, left_on=Attribute_Name, right_on=Set_Name)
    return merged_df.iloc[:, 1:].to_numpy(dtype=float)


	
	
import numpy as np

def Coord_Converter(coords_drug2, nn):
    """
    Converts a numerical coordinate matrix into a formatted feature representation.

    This function takes a `nn x nn` grid (`coords_drug2`) where each entry represents
    a feature index and converts it into a matrix of string labels, where each entry 
    is prefixed with `"F"` to indicate a feature (e.g., `"F1"`, `"F2"`).

    Parameters:
    -----------
    coords_drug2 : numpy.ndarray
        A 2D NumPy array of shape `(nn, nn)` containing numerical feature indices.
    nn : int
        The size of the grid (both width and height of the `nn x nn` matrix).

    Returns:
    --------
    coords_drug3 : numpy.ndarray
        A 2D NumPy array of shape `(nn, nn)`, where each entry is a string label
        in the format `"F{feature_index}"`. If an entry in `coords_drug2` is `NaN`,
        it remains unchanged.

    Algorithm:
    ----------
    1. Initialize an empty `nn x nn` array (`coords_drug3`) with `'NaN'` as string values.
    2. Iterate through each row (`i`) and column (`j`) of `coords_drug2`:
       - Convert the numerical feature index to a string prefixed with `"F"`.
       - Store the formatted string in `coords_drug3[i, j]`.
    3. Return the transformed coordinate matrix.

    Example:
    --------
    >>> coords_drug2 = np.array([[1, 2], [3, 4]])
    >>> nn = 2
    >>> coords_drug3 = Coord_Converter(coords_drug2, nn)
    >>> print(coords_drug3)
    [['F1' 'F2']
     ['F3' 'F4']]

    Notes:
    ------
    - The function ensures all feature indices are converted to a readable format.
    - If `coords_drug2` contains invalid values (e.g., `NaN`), ensure they are handled properly.
    - This function is useful for feature mapping in image-based machine learning models.
    """

    # Initialize a string matrix filled with 'NaN'
    coords_drug3 = np.full((nn, nn), 'NaN', dtype=object)

    # Iterate through the matrix and convert numerical feature indices to string labels
    for i in range(nn):
        for j in range(nn):
            ft = 'F' + str(coords_drug2[i, j])  # Convert number to "F{index}"
            coords_drug3[i, j] = ft  

    return coords_drug3

	


def Bias_Calc(Y_Test, Y_Pred):
    """
    Calculates the bias in predictions by fitting a linear regression model.

    This function computes the bias by fitting a linear regression model 
    where the independent variable is the true values (`Y_Test`) and 
    the dependent variable is the error (`Y_Test - Y_Pred`). The bias 
    is defined as the slope (`coef_`) of the fitted regression model.

    Parameters:
    -----------
    Y_Test : numpy.ndarray
        A 1D NumPy array representing the ground truth (actual values).
    Y_Pred : numpy.ndarray
        A 1D NumPy array representing the predicted values.

    Returns:
    --------
    Bias : float
        The regression coefficient representing the bias in predictions.
        - A positive bias means the model tends to over-predict.
        - A negative bias means the model tends to under-predict.

    Algorithm:
    ----------
    1. Compute the prediction error: `Error = Y_Test - Y_Pred`.
    2. Reshape `Y_Test` and `Error` into 2D arrays to fit `LinearRegression`.
    3. Fit a linear regression model: `Error ~ Y_Test`.
    4. Extract the regression coefficient (`coef_`), which represents the bias.
    5. Return the computed bias.

    Example:
    --------
    >>> Y_Test = np.array([3.0, 4.5, 5.2, 6.8])
    >>> Y_Pred = np.array([2.8, 4.2, 5.5, 7.0])
    >>> Bias = Bias_Calc(Y_Test, Y_Pred)
    >>> print(Bias)
    [0.02]

    Notes:
    ------
    - The bias represents systematic errors in prediction.
    - A bias of zero indicates an unbiased prediction model.
    - The function assumes that `Y_Test` and `Y_Pred` have the same length.
    """

    # Compute the prediction error
    Error = Y_Test - Y_Pred

    # Reshape Y_Test and Error for linear regression
    Y_Test = Y_Test.reshape(len(Y_Test), 1)
    Error = Error.reshape(len(Error), 1)

    # Fit a linear regression model
    reg = LinearRegression().fit(Y_Test, Error)

    # Extract the bias (regression coefficient)
    Bias = reg.coef_[0]

    return Bias

	
